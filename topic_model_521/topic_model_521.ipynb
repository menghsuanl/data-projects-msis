{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d3fd784",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1eb50ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f7c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"He is a good guy, he is not bad\"\n",
    "d2 = \"feet wolves cooked boys girls ,!<@!\"\n",
    "d3 = \"He is not a good guy, he is bad\"\n",
    "d4 = \"I drink water in parties\"\n",
    "d5 = \"I grab a drink in parties\"\n",
    "d6 = \"Seattle weather is bad in winter\"\n",
    "d7 = \"Seattle Seahawks is a great football team\"\n",
    "d8 = \"I love Seahawks\"\n",
    "d9 = \"I learned a lot of Data analytics tools\"\n",
    "d10 = \"I am a data scientist\"\n",
    "c4 = [d1,d2,d3,d4,d5,d6,d7,d8,d9,d10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6acc0024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good guy bad', 'foot wolf cooked boy girl', 'good guy bad', 'drink water party', 'grab drink party', 'seattle weather bad winter', 'seattle seahawks great football team', 'love seahawks', 'learned lot data analytics tool', 'data scientist']\n"
     ]
    }
   ],
   "source": [
    "# data normalization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "#stemmer = nltk.stem.PorterStemmer()\n",
    "processed_c4 = []\n",
    "for doc in c4:\n",
    "    tokens = nltk.word_tokenize(doc.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "   # tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if not token in stopwords.words('english')]\n",
    "    processed_c4.append(\" \".join(tokens))\n",
    "print(processed_c4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "171328ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6169ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4007734  0.         0.         0.45808861 0.45808861 0.45808861\n",
      "  0.45808861 0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.4007734  0.         0.         0.45808861 0.45808861 0.45808861\n",
      "  0.45808861 0.         0.         0.        ]\n",
      " [0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.70710678 0.         0.        ]\n",
      " [0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.70710678 0.         0.        ]\n",
      " [0.65845424 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.75262077]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.70710678 0.70710678]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         1.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         1.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df = 2)\n",
    "vectorizer.fit(processed_c4)\n",
    "X = vectorizer.transform(processed_c4)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23a510",
   "metadata": {},
   "source": [
    "## Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2ed7695e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70710678]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import*\n",
    "# see similarity between 6 & 7\n",
    "cosine_similarity(X[6], X[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "854e406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 3, 3, 2, 2, 2, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = KMeans(n_clusters = 4)\n",
    "model.fit(X)\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1a1f2460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.4007734 , 0.        , 0.        , 0.45808861, 0.45808861,\n",
       "        0.45808861, 0.45808861, 0.        , 0.        , 0.        ],\n",
       "       [0.16461356, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.4267767 , 0.36493189],\n",
       "       [0.        , 0.        , 0.70710678, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.70710678, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interpret the clusters, look at the clusters based on its most freq words\n",
    "# tfidf matrix of 4 cluster centers\n",
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b3ca34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 9, 8, 7, 6, 5, 4, 3, 2, 0],\n",
       "       [6, 5, 4, 3, 0, 9, 8, 7, 2, 1],\n",
       "       [8, 9, 0, 7, 6, 5, 4, 3, 2, 1],\n",
       "       [7, 2, 9, 8, 6, 5, 4, 3, 1, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the index in descending order\n",
    "ordered_centroids = model.cluster_centers_.argsort()[:, ::-1] # all clusters, all index, and stride = -1, meaning come backward\n",
    "\n",
    "ordered_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de245ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m terms \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[1;32m      2\u001b[0m terms[\u001b[38;5;241m8\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names() # or get_feature_names_out()\n",
    "terms[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b306c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "data\n",
      "seattle\n",
      "seahawks\n",
      "party\n",
      "Cluster 1:\n",
      "guy bad\n",
      "guy\n",
      "good guy\n",
      "good\n",
      "Cluster 2:\n",
      "seahawks\n",
      "seattle\n",
      "bad\n",
      "party\n",
      "Cluster 3:\n",
      "party\n",
      "drink\n",
      "seattle\n",
      "seahawks\n"
     ]
    }
   ],
   "source": [
    "for c in range(4):\n",
    "    print('Cluster %d:' %c)\n",
    "    for i in ordered_centroids[c, :4]: # get the first 4 terms of each cluster\n",
    "        print(terms[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad85790",
   "metadata": {},
   "source": [
    "# LDA for topic modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b1ca975",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce81c42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=4).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9941a23d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.06345727, 0.25023589, 0.2502491 , 1.16543301, 1.16543301,\n",
       "        1.16543301, 1.16543301, 0.2502491 , 0.25055307, 0.25029527],\n",
       "       [0.25143948, 0.2507643 , 0.25081188, 0.25044671, 0.25044671,\n",
       "        0.25044671, 0.25044671, 0.25081188, 0.25183149, 0.25083688],\n",
       "       [0.89454113, 0.25020076, 1.66283124, 0.25012054, 0.25012054,\n",
       "        0.25012054, 0.25012054, 1.66283124, 0.92590418, 1.7081386 ],\n",
       "       [0.25056316, 2.24879906, 0.25032135, 0.25017695, 0.25017695,\n",
       "        0.25017695, 0.25017695, 0.25032135, 1.27881804, 0.2504568 ]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model lda's attribute components_ stores topic word distribution. The array components_[i, j] \n",
    "# can be viewed as pseudocount that represents the number of times word j was assigned to topic i.\n",
    "lda.components_ # counts(prob) of word j to appear in topic i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "89658b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "guy bad\n",
      "guy\n",
      "good guy\n",
      "good\n",
      "Topic 1:\n",
      "seahawks\n",
      "bad\n",
      "seattle\n",
      "party\n",
      "Topic 2:\n",
      "seattle\n",
      "party\n",
      "drink\n",
      "seahawks\n",
      "Topic 3:\n",
      "data\n",
      "seahawks\n",
      "bad\n",
      "seattle\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:' % i )\n",
    "    for j in topic.argsort()[:-4-1:-1]:\n",
    "        print(terms[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1fdf8218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "guy bad\n",
      "guy\n",
      "good guy\n",
      "good\n",
      "Topic 1:\n",
      "seahawks\n",
      "bad\n",
      "seattle\n",
      "party\n",
      "Topic 2:\n",
      "seattle\n",
      "party\n",
      "drink\n",
      "seahawks\n",
      "Topic 3:\n",
      "data\n",
      "seahawks\n",
      "bad\n",
      "seattle\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:' % i )\n",
    "    for j in topic.argsort()[:-5:-1]:\n",
    "        print(terms[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f1c7accb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "guy bad\n",
      "guy\n",
      "good guy\n",
      "Topic 1:\n",
      "seahawks\n",
      "bad\n",
      "seattle\n",
      "Topic 2:\n",
      "seattle\n",
      "party\n",
      "drink\n",
      "Topic 3:\n",
      "data\n",
      "seahawks\n",
      "bad\n"
     ]
    }
   ],
   "source": [
    "for i, topic in enumerate(lda.components_):\n",
    "    print('Topic %d:' % i )\n",
    "    for j in topic.argsort()[:-4:-1]:\n",
    "        print(terms[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "10f581e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11078412, 0.10427414, 0.68102396, 0.10391778],\n",
       "       [0.10372815, 0.10413175, 0.67214944, 0.11999066],\n",
       "       [0.12512807, 0.12542264, 0.12922078, 0.62022851]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.transform(X[5:8]) # the prob of topic to appear in doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad2939",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cffa5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f19a2495",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/menghsuanlee/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddc0149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"They are smart, cute, and funny.\",  # positive sentence example\n",
    "    \"They are smart, cute, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "    \"They are very smart, cute, and funny.\",# booster words handled correctly (sentiment intensity adjusted)\n",
    "    \"They are VERY SMART, cute, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "    \"They are VERY SMART, cute, and FUNNY!!!\",  # combination of signals - VADER appropriately adjusts intensity\n",
    "    \"They are VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",  # booster words & punctuation make this close to ceiling for \n",
    "    \"The book was good.\",  # positive sentence\n",
    "    \"The book was kind of good.\",  # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "    \"The plot was good, but the characters are uncompelling and the dialog is not great.\",  # mixed negation sentence\n",
    "    \"A really bad, horrible book.\",  # negative sentence with booster words\n",
    "    \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n",
    "    \":) and :D\",  # emoticons handled\n",
    "    \"\",  # an empty string is correctly handled\n",
    "    \"Today sux\",  # negative slang handled\n",
    "    \"Today sux!\",  # negative slang with punctuation emphasis handled\n",
    "    \"Today SUX!\",  # negative slang with capitalization emphasis\n",
    "    \"Today kinda sux! But I'll get by, lol\" # mixed sentiment example with slang and contrastive conjunction \"but\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "516dbb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They are smart, cute, and funny.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.8225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They are smart, cute, and funny!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.8356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They are very smart, cute, and funny.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.696</td>\n",
       "      <td>0.8470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>They are VERY SMART, cute, and FUNNY.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.9196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They are VERY SMART, cute, and FUNNY!!!</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.9318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>They are VERY SMART, really handsome, and INCR...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.9469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The book was good.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.4404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The book was kind of good.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.657</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.3832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The plot was good, but the characters are unco...</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A really bad, horrible book.</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.8211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>At least it isn't a horrible book.</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.637</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.4310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>:) and :D</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.7925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Today sux</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Today sux!</td>\n",
       "      <td>0.736</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.4199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Today SUX!</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.5461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Today kinda sux! But I'll get by, lol</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.5249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence    neg    neu    pos  \\\n",
       "0                    They are smart, cute, and funny.  0.000  0.259  0.741   \n",
       "1                    They are smart, cute, and funny!  0.000  0.252  0.748   \n",
       "2               They are very smart, cute, and funny.  0.000  0.304  0.696   \n",
       "3               They are VERY SMART, cute, and FUNNY.  0.000  0.249  0.751   \n",
       "4             They are VERY SMART, cute, and FUNNY!!!  0.000  0.236  0.764   \n",
       "5   They are VERY SMART, really handsome, and INCR...  0.000  0.294  0.706   \n",
       "6                                  The book was good.  0.000  0.508  0.492   \n",
       "7                          The book was kind of good.  0.000  0.657  0.343   \n",
       "8   The plot was good, but the characters are unco...  0.327  0.579  0.094   \n",
       "9                        A really bad, horrible book.  0.791  0.209  0.000   \n",
       "10                 At least it isn't a horrible book.  0.000  0.637  0.363   \n",
       "11                                          :) and :D  0.000  0.124  0.876   \n",
       "12                                                     0.000  0.000  0.000   \n",
       "13                                          Today sux  0.714  0.286  0.000   \n",
       "14                                         Today sux!  0.736  0.264  0.000   \n",
       "15                                         Today SUX!  0.779  0.221  0.000   \n",
       "16              Today kinda sux! But I'll get by, lol  0.138  0.517  0.344   \n",
       "\n",
       "    compound  \n",
       "0     0.8225  \n",
       "1     0.8356  \n",
       "2     0.8470  \n",
       "3     0.9196  \n",
       "4     0.9318  \n",
       "5     0.9469  \n",
       "6     0.4404  \n",
       "7     0.3832  \n",
       "8    -0.7042  \n",
       "9    -0.8211  \n",
       "10    0.4310  \n",
       "11    0.7925  \n",
       "12    0.0000  \n",
       "13   -0.3612  \n",
       "14   -0.4199  \n",
       "15   -0.5461  \n",
       "16    0.5249  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "scores = []\n",
    "for s in sentences:\n",
    "    score = {'sentence': s}\n",
    "    score.update(sid.polarity_scores(s))\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
